<!DOCTYPE html><html lang="en"><head><meta name="generator" content="React Static"/><title data-react-helmet="true">Klas Leino</title><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5, shrink-to-fit=no"/><link rel="preload" as="script" href="/templates/styles.33a381e9.js"/><link rel="preload" as="script" href="/templates/vendors~main.d954b5f8.js"/><link rel="preload" as="script" href="/main.74950553.js"/><link rel="preload" as="style" href="/styles.f3106a86.css"/><link rel="stylesheet" href="/styles.f3106a86.css"/><link data-react-helmet="true" href="https://fonts.googleapis.com/css?family=Cuprum" rel="stylesheet"/><link data-react-helmet="true" href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet"/><link data-react-helmet="true" href="https://fonts.googleapis.com/css?family=IBM+Plex+Mono:300" rel="stylesheet"/><link data-react-helmet="true" rel="icon" type="image/x-icon" href="favicon/favicon.ico"/></head><body><div id="root"><div style="outline:none" tabindex="-1"></div></div><script type="text/javascript">window.__routeInfo = JSON.parse("{\"template\":\"__react_static_root__/src/pages/index.jsx\",\"sharedHashesByProp\":{},\"data\":{\"paperIdsWithInfo\":{\"black22ensemble\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Selective Ensembles for Consistent Predictions\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Emily Black\\\",\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"ICLR\\\",\\n\\t\\\"year\\\": \\\"2022\\\",\\n\\t\\\"date\\\": \\\"2022/01/01\\\",\\n\\t\\\"tldr\\\": \\\"This work introduces *selective ensembles* which mitigates the problem of model duplicity by applying hypothesis testing to the predictions of a set of models trained using randomly-selected starting conditions; importantly, selective ensembles can abstain in cases where a consistent outcome cannot be achieved up to a specified confidence level.\\\",\\n\\t\\\"keywords\\\": [\\\"Machine Learning\\\"],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/2111.08230.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Recent work has shown that models trained to the same objective, and which achieve similar measures of accuracy on consistent test data, may nonetheless behave very differently on individual predictions. This inconsistency is undesirable in high-stakes contexts, such as medical diagnosis and finance. We show that this inconsistent behavior extends beyond predictions to feature attributions, which may likewise have negative implications for the intelligibility of a model, and one\u2019s ability to find recourse for subjects. We then introduce <em>selective ensembles</em> to mitigate such inconsistencies by applying hypothesis testing to the predictions of a set of models trained using randomly-selected starting conditions; importantly, selective ensembles can abstain in cases where a consistent outcome cannot be achieved up to a specified confidence level. We prove that that prediction disagreement between selective ensembles is bounded, and empirically demonstrate that selective ensembles achieve consistent predictions and feature attributions while maintaining low abstention rates. On several benchmark datasets, selective ensembles reach zero inconsistently predicted points, with abstention rates as low 1.5%.</p>\\n\"}},\"fromherz21projections\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Fast Geometric Projections for Local Robustness Certification\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Aymeric Fromherz\\\",\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Bryan Parno\\\", \\n\\t\\t\\\"Corina P\u0103s\u0103reanu\\\"\\n\\t],\\n\\t\\\"displayAuthors\\\": [\\n\\t\\t\\\"Klas Leino*\\\",\\n\\t\\t\\\"Aymeric Fromherz*\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Bryan Parno\\\", \\n\\t\\t\\\"Corina P\u0103s\u0103reanu\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"ICLR\\\",\\n\\t\\\"year\\\": \\\"2021\\\",\\n\\t\\\"date\\\": \\\"2021/01/01\\\",\\n\\t\\\"tldr\\\": \\\"This work contributes a post hoc robustness-certification algorithm that operates by efficienntly analyzing the linear regions of ReLU networks with *geometric projections* instead of expensive constraint solving, thus admitting an efficient, highly-parallel GPU implementation at the price of incompleteness, which can be addressed by falling back on prior approaches.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Adversarial Robustness\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/2002.04742.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://github.com/klasleino/fast-geometric-projections\\\",\\n\\t\\t\\t\\\"text\\\": \\\"code\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/iclr21_poster.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"poster\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/iclr21_slides.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"slides\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Local robustness ensures that a model classifies all inputs within an \u03B5-ball consistently, which precludes various forms of adversarial inputs. In this paper, we present a fast procedure for checking local robustness in feed-forward neural networks with piecewise linear activation functions. The key insight is that such networks partition the input space into a polyhedral complex such that the network is linear inside each polyhedral region; hence, a systematic search for decision boundaries within the regions around a given input is sufficient for assessing robustness. Crucially, we show how these regions can be analyzed using geometric projections instead of expensive constraint solving, thus admitting an efficient, highly-parallel GPU implementation at the price of incompleteness, which can be addressed by falling back on prior approaches. Empirically, we find that incompleteness is not often an issue, and that our method performs one to two orders of magnitude faster than existing robustness-certification techniques based on constraint solving.</p>\\n\"}},\"leino19bias_amp\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Feature-wise Bias Amplification\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Emily Black\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"ICLR\\\",\\n\\t\\\"year\\\": \\\"2019\\\",\\n\\t\\\"date\\\": \\\"2019/05/01\\\",\\n\\t\\\"tldr\\\": \\\"This work demonstrates that a mismatch between the prior and predicted class distributions can arise from an apparent inductive bias that causes a model to overestimate the importance of moderately predictive \\\\\\\"weak\\\\\\\" features, when insufficient training data is available.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"Machine Learning\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/1812.08999.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/iclr19_poster\\\",\\n\\t\\t\\t\\\"text\\\": \\\"poster\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>We study the phenomenon of <em>bias amplification</em> in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate that bias amplification can arise via an inductive bias in gradient descent methods that results in the overestimation of the importance of moderately-predictive \u201Cweak\u201D features if insufficient training data is available. This overestimation gives rise to <em>feature-wise bias amplification</em>&mdash;a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, we show that while some bias cannot be mitigated without sacrificing accuracy, feature-wise bias amplification can be mitigated through targeted feature selection. We present two new feature selection algorithms for mitigating bias amplification in linear models, and show how they can be adapted to convolutional neural networks efficiently. Our experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy.</p>\\n\"}},\"leino18influence\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Influence-directed Explanations for Convolutional Neural Networks\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Shayak Sen\\\",\\n\\t\\t\\\"Anupam Datta\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"IEEE ITC\\\",\\n\\t\\\"year\\\": \\\"2018\\\",\\n\\t\\\"date\\\": \\\"2018/01/01\\\",\\n\\t\\\"tldr\\\": \\\"This work introduces a framework for explaining neural network behavior through computing axiomatically justified influence scores for neurons in the network. Our framework is uniquely general, giving more flexibility and power to probe a model's behavior, including its internal feature representation.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"Explainable AI\\\",\\n\\t\\t\\\"Computer Vision\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/1802.03788.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://deep.ghost.io/explaining-deep-neural-networks-via-internal-influence/\\\",\\n\\t\\t\\t\\\"text\\\": \\\"blog post\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://github.com/truera/trulens\\\",\\n\\t\\t\\t\\\"text\\\": \\\"TruLens\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our <em>influence-directed explanations</em> approach this problem by peering inside the network to identify neurons with high <em>influence</em> on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an <em>interpretation</em> for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the \u201Cessence\u201D of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.</p>\\n\"}},\"leino20membership\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"USENIX\\\",\\n\\t\\\"year\\\": \\\"2020\\\",\\n\\t\\\"date\\\": \\\"2020/10/01\\\",\\n\\t\\\"tldr\\\": \\\"This work demonstrates how a model's idiosyncratic use of features can leak information about its training data to an attacker with access to the model's parameters&mdash;even when the model's black-box behavior appears to generalize well. We use our analysis to derive a white-box *membership inference* attack that this attack outperforms prior black-box methods.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Data Privacy\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/1906.11798.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://medium.com/towards-data-science/overfitting-and-conceptual-soundness-3a1fd187a6d4\\\",\\n\\t\\t\\t\\\"text\\\": \\\"blog post\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/usenix20_slides.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"slides\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://www.youtube.com/watch?v=jMFl-EF4Nlk\\\",\\n\\t\\t\\t\\\"text\\\": \\\"video\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p><em>Membership inference</em> (MI) attacks exploit a learned model\u2019s lack of generalization to infer whether a given sample was in the model\u2019s training set.\\nKnown MI attacks generally work by casting the attacker\u2019s goal as a supervised learning problem, training an attack model from predictions generated by the target model, or by others like it.\\nHowever, we find that these attacks do not often provide a meaningful basis for confidently inferring training set membership, as the attack models are not well-calibrated.\\nMoreover, these attacks do not significantly outperform a trivial attack that predicts that a point is a member if and only if the model correctly predicts its label.</p>\\n<p>In this work we present well-calibrated MI attacks that allow the attacker to accurately control the minimum confidence with which positive membership inferences are made.\\nOur attacks take advantage of white-box information about the target model and leverage new insights about how overfitting occurs in deep neural networks; namely, we show how a model\u2019s idiosyncratic use of features can provide evidence for membership. \\nExperiments on seven real-world datasets show that our attacks support calibration for high-confidence inferences, while outperforming previous MI attacks in terms of accuracy.\\nFinally, we show that our attacks achieve non-trivial advantage on some models with low generalization error, including those trained with small-epsilon-differential privacy; for large-epsilon (epsilon=16, as reported in some industrial settings), the attack performs comparably to unprotected models.</p>\\n\"}},\"leino21relaxing\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Relaxing Local Robustness\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"NIPS\\\",\\n\\t\\\"year\\\": \\\"2021\\\",\\n\\t\\\"date\\\": \\\"2021/05/15\\\",\\n\\t\\\"tldr\\\": \\\"This work introduces two relaxed safety properties for classifiers that address potential weaknesses in the traditional notion of local robustness: (1) relaxed top-k robustness, which serves as the analogue of top-k accuracy; and (2) affinity robustness, which specifies which sets of labels must be separated by a robustness margin, and which can be close in Lp space.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Adversarial Robustness\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/2106.06624.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://github.com/klasleino/gloro/tree/master/gloro/relaxations\\\",\\n\\t\\t\\t\\\"text\\\": \\\"code\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/nips21_poster.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"poster\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/nips21_slides.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"slides\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Certifiable <em>local robustness</em>, which rigorously precludes small-norm <em>adversarial examples</em>, has received significant attention as a means of addressing security concerns in deep learning. However, for some classification problems, local robustness is not a natural objective, even in the presence of adversaries; for example, if an image contains two classes of subjects, the correct label for the image may be considered arbitrary between the two, and thus enforcing strict separation between them is unnecessary. In this work, we introduce two relaxed safety properties for classifiers that address this observation: (1) <em>relaxed top-k robustness</em>, which serves as the analogue of top-k accuracy; and (2) <em>affinity</em> robustness, which specifies which sets of labels must be separated by a robustness margin, and which can be \u03B5-close in \u2113p space. We show how to construct models that can be efficiently certified against each relaxed robustness property, and trained with very little overhead relative to standard gradient descent. Finally, we demonstrate experimentally that these relaxed variants of robustness are well-suited to several significant classification problems, leading to lower rejection rates and higher certified accuracies than can be obtained when certifying \u201Cstandard\u201D local robustness.</p>\\n\"}},\"leino22degradation\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Degradation Attacks on Certifiably Robust Neural Networks\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Ravi Mangal\\\",\\n\\t\\t\\\"Chi Zhang\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Bryan Parno\\\", \\n\\t\\t\\\"Corina P\u0103s\u0103reanu\\\"\\n\\t],\\n\\t\\\"displayAuthors\\\": [\\n\\t\\t\\\"Klas Leino*\\\",\\n\\t\\t\\\"Ravi Mangal*\\\",\\n\\t\\t\\\"Chi Zhang*\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Bryan Parno\\\", \\n\\t\\t\\\"Corina P\u0103s\u0103reanu\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"TMLR\\\",\\n\\t\\\"year\\\": \\\"2022\\\",\\n\\t\\\"date\\\": \\\"2022/11/01\\\",\\n\\t\\\"tldr\\\": \\\"Adversarial examples are generally understood as being derived from perturbing benign inputs. However, defenses that guard against adversarial examples do not have such benign inputs as a reference point; we exploit this fact to create an attack that causes certified models to reject non-adversarial inputs, degrading their performance.\\\",\\n\\t\\\"keywords\\\": [\\\"ML Security\\\", \\\"Adversarial Robustness\\\"],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://openreview.net/pdf?id=P0XO5ZE98j\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Certifiably robust neural networks protect against adversarial examples by employing run-time defenses that check if the model is certifiably locally robust at the input under evaluation. We show through examples and experiments that any defense (whether complete or incomplete) based on checking local robustness is inherently over-cautious. Specifically, such defenses flag inputs for which local robustness checks fail, but yet that are not adversarial; i.e., they are classified consistently with all valid inputs within a distance of \u03B5. As a result, while a norm-bounded adversary cannot change the classification of an input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. We empirically demonstrate the efficacy of such attacks against state-of-the-art certifiable defenses.</p>\\n\"}},\"leino21gloro\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Globally Robust Neural Networks\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Zifan Wang\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"ICML\\\",\\n\\t\\\"year\\\": \\\"2021\\\",\\n\\t\\\"date\\\": \\\"2021/02/01\\\",\\n\\t\\\"tldr\\\": \\\"This work demonstrates how Lipschitz bounds can be effectively incorporated into a natural learning objective to train \\\\\\\"globally robust\\\\\\\" models, which can be efficiently certified against adversarial input perturbations while achieving state-of-the-art certified performance.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Adversarial Robustness\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/2102.08452.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://towardsdatascience.com/training-provably-robust-neural-networks-1e15f2d80be2\\\",\\n\\t\\t\\t\\\"text\\\": \\\"blog post\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://github.com/klasleino/gloro\\\",\\n\\t\\t\\t\\\"text\\\": \\\"code\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/icml21_poster.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"poster\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/icml21_slides.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"slides\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>The threat of adversarial examples has motivated work on training certifiably robust neural networks, to facilitate efficient verification of local robustness at inference time. We formalize a notion of global robustness, which captures the operational properties of on-line local robustness certification while yielding a natural learning objective for robust training. We show that widely-used architectures can be easily adapted to this objective by incorporating efficient global Lipschitz bounds into the network, yielding certifiably-robust models by construction that achieve state-of-the-art verifiable accuracy. Notably, this approach requires significantly less time and memory than recent certifiable training methods, and leads to negligible costs when certifying points on-line; for example, our evaluation shows that it is possible to train a large tiny-imagenet model in a matter of hours. We posit that this is possible using inexpensive global bounds\u2014despite prior suggestions that tighter local bounds are needed for good performance\u2014because these models are trained to achieve tighter global bounds. Namely, we prove that the maximum achievable verifiable accuracy for a given dataset is not improved by using a local bound.</p>\\n\"}},\"leino22repair\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Self-Correcting Neural Networks for Safe Classification\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Aymeric Fromherz\\\",\\n\\t\\t\\\"Ravi Mangal\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Bryan Parno\\\", \\n\\t\\t\\\"Corina P\u0103s\u0103reanu\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"FoMLAS\\\",\\n\\t\\\"year\\\": \\\"2022\\\",\\n\\t\\\"date\\\": \\\"2022/01/01\\\",\\n\\t\\\"tldr\\\": \\\"This work develops a run-time mechanism for the enforcement of a broad class of \\\\\\\"safe-ordering\\\\\\\" constraints for neural networks, based on a *self-correcting layer*, which provably yields safe outputs regardless of the characteristics of its input, while provably preserving safe classificaion accuracy.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Safety Verification\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/2107.11445.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://github.com/cmu-transparency/self-correcting-networks\\\",\\n\\t\\t\\t\\\"text\\\": \\\"code\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Classifiers learnt from data are increasingly being used as components in systems where safety is a critical concern. In this work, we present a formal notion of safety for classifiers via constraints called <em>safe-ordering constraints</em>. These constraints relate requirements on the order of the classes output by a classifier to conditions on its input, and are expressive enough to encode various interesting examples of classifier safety specifications from the literature. For classifiers implemented using neural networks, we also present a run-time mechanism for the enforcement of safe-ordering constraints. Our approach is based on a <em>self-correcting layer</em>, which provably yields safe outputs regardless of the characteristics of the classifier input. We compose this layer with an existing neural network classifier to construct a <em>self-correcting network</em> (SC-Net), and show that in addition to providing safe outputs, the SC-Net is guaranteed to preserve the classification accuracy of the original network whenever possible. Our approach is independent of the size and architecture of the neural network used for classification, depending only on the specified property and the dimension of the network\u2019s output; thus it is scalable to large state-of-the-art networks. We show that our approach can be optimized for a GPU, introducing run-time overhead of less than 1ms on current hardware-even on large, widely-used networks containing hundreds of thousands of neurons and millions of parameters.</p>\\n\"}},\"leino22thesis\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Identifying, Analyzing, and Addressing Weaknesses in Deep Networks: Foundations for Conceptually Sound Neural Networks\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\"\\n\\t],\\n\\t\\\"year\\\": \\\"2022\\\",\\n\\t\\\"date\\\": \\\"2022/03/08\\\",\\n\\t\\\"tldr\\\": \\\"Despite their remarkable abilities, neural networks have several peculiar shortcomings and vulnerabilities, many of which relate to a lack of *conceptual soundness* in the features encoded and used by the network&mdash;that is, the features the network learns to use may represent concepts that are not appropriate for the task at hand, even when they apparently allow the network to perform well on previously unseen validation data. This thesis examines the problems that arise in deep networks when they are not sufficiently conceptually sound, and provides steps towards improving the conceptual soundness of modern networks.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"Machine Learning\\\",\\n\\t\\t\\\"Explainable AI\\\",\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Adversarial Robustness\\\",\\n\\t\\t\\\"Data Privacy\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/thesis.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"pdf\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/thesis_slides.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"slides\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Deep neural networks have seen great success in many domains, with the ability to perform complex human tasks such as image recognition, text translation, and medical diagnosis; however, despite their remarkable abilities, neural networks have several peculiar shortcomings and vulnerabilities.\\nMany of these weaknesses relate to a lack of <em>conceptual soundness</em> in the features encoded and used by the network&mdash;that is, the features the network learns to use may represent concepts that are not appropriate for the task at hand, even when they apparently allow the network to perform well on previously unseen validation data.\\nThis thesis examines the problems that arise in deep networks when they are not sufficiently conceptually sound, and provides steps towards improving the conceptual soundness of modern networks.</p>\\n<p>The first contribution of this thesis is a general, axiomatically justified framework for explaining neural network behavior, which serves as a powerful tool for assessing conceptual soundness.\\nThis work takes the unique perspective that to accurately assess the conceptual soundness of a model, an explanation must provide a <em>faithful</em> account of its behavior.\\nBy contrast, the literature has often attempted to justify explanations based on their appeal to human intuition; however, this begs the question, as it assumes the model captured conceptually sound human intuition in the first place.</p>\\n<p>To the contrary, a large body of prior work provides conclusive evidence that conceptual soundness is <em>not</em> the norm in standard deep networks, as <em>adversarial examples</em>&mdash;small, semantically meaningless input perturbations that cause erroneous behavior&mdash;found ubiquitously therein, violate the tenets of conceptual soundness.\\nThe second part of this thesis addresses this issue by contributing a state-of-the-art method for training neural networks with <em>provable guarantees</em> against a common class of adversarial examples.</p>\\n<p>Finally, we demonstrate that robustness to malicious input perturbations is only the first step&mdash;with contributions uncovering several orthogonal weaknesses and vulnerabilities relating to the conceptual soundness of deep networks.</p>\\n\"}},\"lu20influence\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Kaiji Lu\\\", \\n\\t\\t\\\"Piotr Mardziel\\\",\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Anupam Datta\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"ACL\\\",\\n\\t\\\"year\\\": \\\"2020\\\",\\n\\t\\\"date\\\": \\\"2020/01/01\\\",\\n\\t\\\"tldr\\\": \\\"This work introduces a framework for explaining LSTM language model behavior through a causal account of how influence is carried across gates and neurons of a recurrent neural network, and uses it to analyze how common LSTM models handle subject-verb number agreement.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"Explainable AI\\\",\\n\\t\\t\\\"Natural Language Processing\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://arxiv.org/pdf/2005.01190.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"arXiv\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTMs on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce <em>influence paths</em>, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject\u2019s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate-level or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-level LSTM language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an LSTM\u2019s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.</p>\\n\"}},\"leino22privacy\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Robust Features Can Leak Instances and Their Properties\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Jonathan Helland\\\",\\n\\t\\t\\\"Saranya Vijaykumar\\\",\\n\\t\\t\\\"Anusha Sinha\\\",\\n\\t\\t\\\"Nathan VanHoudnos\\\",\\n\\t\\t\\\"Matt Fredrikson\\\"\\n\\t],\\n\\t\\\"year\\\": \\\"2022\\\",\\n\\t\\\"date\\\": \\\"2022/12/01\\\",\\n\\t\\\"tldr\\\": \\\"This work shows that models trained to be robust to adversarial examples may, in practice, be more vulnerable to leaking private information about their training sets, particularly to low-information adversaries. In some cases, models may catastrophically leak _entire training instances_, or distributional properties orthogonal to the model's learning objective.\\\",\\n\\t\\\"keywords\\\": [\\n\\t\\t\\\"ML Security\\\",\\n\\t\\t\\\"Data Privacy\\\",\\n\\t\\t\\\"Adversarial Robustness\\\"\\n\\t],\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"pdf/privacy-and-robustness.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"pdf\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>Previous work has shown that neural networks are prone to leaking private information about their training data. While privacy is typically associated with features of specific training points, we argue that when a feature is independent of a model\u2019s learning objective, and thus irrelevant to its prediction task, one may reasonably expect the model not to disclose it. To this end, we explore the extent to which such irrelevant features may be encoded and leaked by a neural network, and provide evidence that this type of leakage occurs. Meanwhile, so-called robust models that are protected against a seemingly orthogonal model integrity threat&mdash;adversarial examples&mdash;have been found to lead to networks that encode more human-interpretable features. We show that these findings are connected to the aforementioned privacy risks in neural networks: when a robust network learns features that are overly specific to private properties of its training data, the characteristically high quality feature visualizations associated with robustness may reveal said private properties plainly. We support these findings quantitatively via a novel property inference game that operates on feature visualizations, and find that properties are often leaked through visualizations regardless of whether they can be detected by the human eye. This suggests that the role robustness plays in the leakage is related to the way in which robust models organize private information, rather than the degree to which they encode it. Practically, this means that robust models may be uniquely vulnerable to a weak adversary\u2014in the most egregious cases, we find that training data properties, or even entire training instances, may be directly encoded in a model\u2019s weights, and obtainable to a suitable approximation by a black-box attacker.</p>\\n\"}}},\"tutorialIdsWithInfo\":{\"datta21aaai_tutorial\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"From Explanability to Model Quality and Back Again\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Anupam Datta\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Kaiji Lu\\\",\\n\\t\\t\\\"Shayak Sen\\\",\\n\\t\\t\\\"Zifan Wang\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"AAAI\\\",\\n\\t\\\"year\\\": \\\"2021\\\",\\n\\t\\\"date\\\": \\\"2021/02/01\\\",\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://fairlyaccountable.org/aaai-2021-tutorial/\\\",\\n\\t\\t\\t\\\"text\\\": \\\"webpage\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"slides\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>The goal of this tutorial is to provide a systematic view of the current knowledge relating explainability to several key outstanding concerns regarding the quality of ML models; in particular, robustness, privacy, and fairness. We will discuss the ways in which explainability can inform questions about these aspects of model quality, and how methods for improving them that are emerging from recent research of AI, Security &amp; Privacy, and Fairness communities can in turn lead to better outcomes for explainability. We aim to make these findings accessible to a general AI audience, including not only researchers who want to further engage with this direction, but also practitioners who stand to benefit from the results, and policy-makers who want to deepen their technical understanding of these important issues.</p>\\n\"}},\"datta21trulens\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Exploring Conceptual Soundness with TruLens\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Anupam Datta\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Kaiji Lu\\\",\\n\\t\\t\\\"Shayak Sen\\\",\\n\\t\\t\\\"Ricardo Shih\\\",\\n\\t\\t\\\"Zifan Wang\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"NIPS\\\",\\n\\t\\\"type\\\": \\\"Demo\\\",\\n\\t\\\"year\\\": \\\"2021\\\",\\n\\t\\\"date\\\": \\\"2021/12/01\\\",\\n\\t\\\"publishedIn\\\": \\\"NIPS Competitions and Demonstrations Track\\\",\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://proceedings.mlr.press/v176/datta22a/datta22a.pdf\\\",\\n\\t\\t\\t\\\"text\\\": \\\"PMLR paper\\\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://truera.github.io/neurips-demo-2021/\\\",\\n\\t\\t\\t\\\"text\\\": \\\"webpage\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>As machine learning has become increasingly ubiquitous, there has been a growing need to assess the trustworthiness of learned models. One important aspect to model trust is conceptual soundness, i.e., the extent to which a model uses features that are appropriate for its intended task. We present <em>TruLens</em>, a new cross-platform framework for explaining deep network behavior. In our demonstration, we provide an interactive application built on TruLens that we use to explore the conceptual soundness of various pre-trained models. We take the unique perspective that robustness to small-norm adversarial examples is a necessary condition for conceptual soundness; we demonstrate this by comparing explanations on models trained with and without a robust objective. Our demonstration will focus on our end-to-end application, which will be made accessible for the audience to interact with; but we will also provide details on its open-source components, including the TruLens library and the code used to train robust networks.</p>\\n\"}},\"datta21kdd_tutorial\":{\"infoJson\":{\"contents\":\"{\\n\\t\\\"title\\\": \\\"Machine Learning Explainability and Robustness: Connected at the Hip\\\",\\n\\t\\\"authors\\\": [\\n\\t\\t\\\"Anupam Datta\\\",\\n\\t\\t\\\"Matt Fredrikson\\\",\\n\\t\\t\\\"Klas Leino\\\",\\n\\t\\t\\\"Kaiji Lu\\\",\\n\\t\\t\\\"Shayak Sen\\\",\\n\\t\\t\\\"Zifan Wang\\\"\\n\\t],\\n\\t\\\"conference\\\": \\\"KDD\\\",\\n\\t\\\"year\\\": \\\"2021\\\",\\n\\t\\\"date\\\": \\\"2021/07/01\\\",\\n\\t\\\"links\\\": [\\n\\t\\t{\\n\\t\\t\\t\\\"url\\\": \\\"https://sites.google.com/andrew.cmu.edu/kdd-2021-tutorial-expl-robust/\\\",\\n\\t\\t\\t\\\"text\\\": \\\"webpage\\\"\\n\\t\\t}\\n\\t]\\n}\"},\"abstract\":{\"contents\":\"<p>This tutorial examines the synergistic relationship between explainability methods for machine learning and a significant problem related to model quality: robustness against adversarial perturbations. We begin with a broad overview of approaches to explainable AI, before narrowing our focus to post-hoc explanation methods for predictive models. We discuss perspectives on what constitutes a good explanation in various settings, with an emphasis on axiomatic justifications for various explanation methods. In doing so, we will highlight the importance of an explanation method\u2019s faithfulness to the target model, as this property allows one to distinguish between explanations that are unintelligible because of the method used to produce them, and cases where a seemingly poor explanation points to model quality issues. Next, we introduce concepts surrounding adversarial robustness, including state-of-the-art adversarial attacks as well as a range of corresponding defenses. Finally, building on the knowledge presented thus far, we present key insights from the recent literature on the connections between explainability and adversarial robustness. We show that many commonly-perceived issues in explanations are actually caused by a lack of robustness. At the same time, we show that a careful study of adversarial examples and robustness can lead to models whose explanations better appeal to human intuition and domain knowledge.</p>\\n\"}}}},\"path\":\"/\",\"sharedData\":{},\"siteData\":{}}");</script><script defer="" type="text/javascript" src="/templates/styles.33a381e9.js"></script><script defer="" type="text/javascript" src="/templates/vendors~main.d954b5f8.js"></script><script defer="" type="text/javascript" src="/main.74950553.js"></script></body></html>